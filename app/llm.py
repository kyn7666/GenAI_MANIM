# app/llm.py
import os, json
from typing import Dict, Any, List, Tuple
from dotenv import load_dotenv
from openai import OpenAI
from app.schema import schema_errors, invariants_errors, validate_attention_ir  # ê²€ì¦ì€ ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš©:contentReference[oaicite:2]{index=2}
from app.prompts import DOMAIN_PROMPTS
from app.patterns import PatternType

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ---------- Stage 1: ì´í•´Â·ì˜ˆì‹œÂ·trace ----------
STAGE1_SYSTEM = """You are an algorithm explainer. Output ONLY JSON."""

def build_prompt_stage1(user_text: str) -> str:
    return f"""
Explain the algorithm shortly and produce a small concrete example with a step-by-step trace.

Output ONLY JSON with keys:
- algorithm: snake_case (e.g., "bubble_sort")
- description: short summary (Korean allowed)
- input: for sorting, use {{"array":[...]}} with length â‰¤ 6
- trace: list of steps. Each step:
  - step: integer
  - compare: [i, j]  # indexes compared
  - swap: boolean
  - array: the full array state AFTER this step
- metadata: include {{"domain":"sorting"}}

TEXT:
{user_text}
""".strip()

def call_llm_stage1(user_text: str) -> Dict[str, Any]:
    prompt = build_prompt_stage1(user_text)
    resp = client.chat.completions.create(
        model="gpt-5",  
        response_format={"type": "json_object"},
        messages=[{"role": "system", "content": STAGE1_SYSTEM},
                  {"role": "user", "content": prompt}],
    )
    return json.loads(resp.choices[0].message.content)

# ---------- Stage 2: trace â†’ IR ----------
STAGE2_SYSTEM = """You convert a trace JSON into an animation-ready IR. Output ONLY JSON with exactly three top-level keys: components, events, metadata."""


def build_prompt_stage2(explain_json: Dict[str, Any]) -> str:
    return f"""
Convert the following trace JSON into an animation IR.

Rules:
- Output must have three top-level keys: components[], events[], metadata{{}}.
- Make one component per input item: ids "arr0","arr1",... label is current value (string)
- For each trace step:
  - Emit: {{"t": T, "op": "compare", "from": "arr<i>", "to": "arr<j>"}}
  - If swap==true, also emit: {{"t": T+0.2, "op": "swap", "from": "arr<i>", "to": "arr<j>"}}
- Use non-decreasing t (start 0.0, step 0.2)
- metadata.view = "flow"; metadata.domain = input.metadata.domain

TRACE JSON:
{json.dumps(explain_json, ensure_ascii=False)}
"""


def call_llm_stage2(explain_json: Dict[str, Any], temperature: float = 0.0) -> Dict[str, Any]:
    prompt = build_prompt_stage2(explain_json)
    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        temperature=temperature,
        response_format={"type": "json_object"},
        messages=[{"role": "system", "content": STAGE2_SYSTEM},
                  {"role": "user", "content": prompt}],
    )
    return json.loads(resp.choices[0].message.content)

# ---------- Validation wrapper ----------
def validate_ir(doc: Dict[str, Any]) -> List[str]:
    """schema + invariants ì˜¤ë¥˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ í†µê³¼)."""
    return schema_errors(doc) + invariants_errors(doc)

def call_llm_json_ir(user_text: str, temperature: float = 0.0):
    """
    [í˜¸í™˜ìš©] ì˜› í•¨ìˆ˜ ì´ë¦„ì„ ìœ ì§€í•˜ë˜, ë‚´ë¶€ì ìœ¼ë¡œ
    1) stage1(ì„¤ëª…+ì˜ˆì‹œ+trace) â†’ 2) stage2(traceâ†’IR) ë¥¼ í˜¸ì¶œí•´ì„œ IRì„ ë§Œë“ ë‹¤.
    ê¸°ì¡´ í˜¸ì¶œë¶€ê°€ (dict, raw_str) ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    explain = call_llm_stage1(user_text, temperature=temperature)
    ir = call_llm_stage2(explain, temperature=temperature)
    raw = json.dumps(ir, ensure_ascii=False)
    return ir, raw

def generate_ir_with_validation(user_text: str, max_retries_zero_temp: int = 2) -> Dict[str, Any]:
    """
    1) temp=0ìœ¼ë¡œ ì‹œë„ â†’ ê²€ì¦ ì‹¤íŒ¨ ì‹œ í”¼ë“œë°± ì²¨ë¶€ ì¬ì‹œë„
    2) ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ temp=0.3ìœ¼ë¡œ í•œ ë²ˆ ë”
    """
    feedback = ""
    for attempt in range(max_retries_zero_temp + 1):
        doc, raw = call_llm_json_ir(user_text + ("\n\n" + feedback if feedback else ""), temperature=0.0)
        errs = schema_errors(doc) + invariants_errors(doc)
        if not errs:
            return doc
        # êµ¬ì²´ì  í”¼ë“œë°± ìƒì„±
        bullets = "\n".join(f"- {e}" for e in errs)
        feedback = f"Correct these issues:\n{bullets}\nReturn valid JSON only."

    # fallback: temperature ë†’ì—¬ì„œ ë‹¤ì–‘ì„± í™•ë³´
    doc, raw = call_llm_json_ir(user_text + ("\n\n" + feedback if feedback else ""), temperature=0.3)
    errs = schema_errors(doc) + invariants_errors(doc)
    if errs:
        raise ValueError("LLM JSON IR generation failed:\n" + "\n".join(errs))
    return doc

# ---------- Domain-level IR Generator ----------
def call_llm_domain_ir(domain: str, user_text: str, temperature: float = 0.0) -> Dict[str, Any]:
    if domain not in DOMAIN_PROMPTS:
        raise ValueError(f"Unknown domain: {domain}")

    prompt_cfg = DOMAIN_PROMPTS[domain]
    template = prompt_cfg["template"]

    # âš ï¸ ì •ë ¬ í…œí”Œë¦¿ì€ JSON ì˜ˆì‹œ ë•Œë¬¸ì— {}ê°€ ë„ˆë¬´ ë§ì•„ì„œ .formatì„ ì“°ë©´ í•­ìƒ í„°ì§„ë‹¤.
    if domain == "sorting_trace":
        # template ì•ˆì— {text} ê°™ì€ placeholderë„ ì“°ì§€ ë§ê³ ,
        # ê·¸ëƒ¥ ë§¨ ì•„ë˜ì— user_textë¥¼ ë¶™ì—¬ì„œ ë³´ë‚´ëŠ” ë°©ì‹ìœ¼ë¡œ ê°„ë‹¤.
        base_prompt = template + f"\n\nUser request:\n{user_text}\n"
    else:
        # cnn_param, seq_attention ìª½ì€ ì›ë˜ëŒ€ë¡œ {text} ì¹˜í™˜ ìœ ì§€
        base_prompt = template.format(text=user_text)

    universal_rules = """
    <GLOBAL RULES>
    - ì ˆëŒ€ë¡œ ì‚¬ìš©ìì˜ ìˆ˜ì¹˜ê°’(ì˜ˆ: 3x3, 2, stride=1, 0.01, learning rate ë“±)ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ë³´ì •í•˜ì§€ ë§ë¼.
    - padding, stride, kernel_size, input_size, epoch, batch_size, temperature ë“±
      ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì…ë ¥ëœ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¼.
    - JSON ì´ì™¸ì˜ ìì—°ì–´ ì„¤ëª…, ì£¼ì„, ì½”ë“œë¸”ë¡ì„ ì¶œë ¥í•˜ì§€ ë§ë¼.
    """

    final_prompt = base_prompt + "\n\n" + universal_rules

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": prompt_cfg["system"]},
            {"role": "user", "content": final_prompt},
        ],
        temperature=temperature,
    )

    print("\n=== ğŸ§  LLM RAW OUTPUT ===")
    print(resp.choices[0].message.content)
    print("=========================\n")

    return json.loads(resp.choices[0].message.content)


def call_llm_attention_ir(user_text: str) -> dict:
    # ë„ë©”ì¸ì€ patternê³¼ 1:1ë¡œ ë§ì¶˜ë‹¤
    raw = call_llm_domain_ir("seq_attention", user_text)
    # rawê°€ ë°”ë¡œ attn_irë¼ê³  ê°€ì • 
    attn_ir = raw

    errors = validate_attention_ir(attn_ir)
    if errors:
        raise ValueError(f"attention IR validation failed: {errors}")
    return attn_ir